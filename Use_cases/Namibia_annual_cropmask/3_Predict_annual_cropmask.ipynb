{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an annual cropmask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Using the model for the `crop_mask_southern` product, this notebook will make predictions on new data to generate a cropland mask for the area defined by a geojson. Results are saved to disk as Cloud-Optimised-Geotiffs.\n",
    "\n",
    "1. Open and inspect the shapefile which delineates the extent we're classifying\n",
    "2. Import the model\n",
    "3. Make predictions on new data loaded through the ODC.  The pixel classification will also undergo a post-processing step where steep slopes and water are masked using a SRTM derivative and WOfS, respectively. Pixels labelled as crop above 3600 metres ASL are also masked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import os\n",
    "import datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from joblib import load\n",
    "from tqdm.auto import tqdm\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.cog import write_cog\n",
    "from datacube.utils.geometry import assign_crs\n",
    "from datacube.testutils.io import rio_slurp_xarray\n",
    "\n",
    "from deafrica_tools.classification import HiddenPrints, predict_xr\n",
    "from deafrica_tools.spatial import xr_rasterize\n",
    "from deafrica_tools.dask import create_local_dask_cluster\n",
    "\n",
    "#import out feature layer function for prediction\n",
    "from feature_layer_functions import gm_mads_two_seasons_prediction\n",
    "from post_processing import post_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "* `aoi`: A path to a geojson define the extent of the analysis area. Make sure each polygon in the geojson has a column with some kind of unique ID\n",
    "* `output_suffix`: Use this as folder and file name for the classifications that are output\n",
    "* `column`: The column header name in the `aoi` geojson/shapefile that contains the unique ID for the polygons in the file \n",
    "* `year`: What year to run the crop mask for.\n",
    "* `resolution`: What pixel resolution to run the classifciaiton at `(-10,10)` is the max resolution. Use `(-20,20)` to have the code run faster and get quick results.\n",
    "* `results`: A folder location to store the classified geotiffs\n",
    "* `model_path`: The path to the location where the model exported from the previous notebook is stored\n",
    "* `training_data`: Name and location of the training data `.txt` file (this has been copied into the data folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi = 'data/Area_of_interest.geojson'\n",
    "\n",
    "output_suffix = 'aoi'\n",
    "\n",
    "column = 'Id'\n",
    "\n",
    "year = '2021'\n",
    "\n",
    "resolution = (-20,20)\n",
    "\n",
    "results = 'results/classifications/'\n",
    "\n",
    "model_path = 'results/southern_ml_model_20220225_2021.joblib'\n",
    "\n",
    "training_data = \"results/southern_training_data_20220225_2021.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and inspect AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(aoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.explore(column=column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(model_path).set_params(n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "model_input = np.loadtxt(training_data)\n",
    "\n",
    "# load the column_names\n",
    "with open(training_data, 'r') as file:\n",
    "    header = file.readline()\n",
    "    \n",
    "column_names = header.split()[1:][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through tiles and predict, with post-processing\n",
    "\n",
    "For every tile we list in the `aoi`, we calculate the feature layers, and then use the DE Africa function `predict_xr` to classify the data.\n",
    "\n",
    "The `feature_layer_functions.gm_mads_two_seasons_prediction` function is doing most of the heavy-lifting here.\n",
    "\n",
    "The results are exported to file as Cloud-Optimised Geotiffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a folder to store results in if one doesn't already exist\n",
    "if not os.path.exists(results+output_suffix):\n",
    "        os.mkdir(results+output_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle dask garbage collections warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "g0, g1, g2 = gc.get_threshold()\n",
    "gc.set_threshold(g0*2, g1*2, g2*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through polygons and predict the crop mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "i=1\n",
    "for index, row in gdf.iterrows():\n",
    "    \n",
    "    print('working on polygon: '+str(i)+\"/\"+str(len(gdf)),end='\\r')\n",
    "    \n",
    "    # Get the geometry\n",
    "    geom = geometry.Geometry(row.geometry.__geo_interface__,\n",
    "                             geometry.CRS(f'EPSG:{gdf.crs.to_epsg()}'))\n",
    "    \n",
    "    #create query \n",
    "    query = {\n",
    "        'geopolygon': geom,\n",
    "        'time': year,\n",
    "        'resolution': resolution,\n",
    "        'output_crs': 'epsg:6933',\n",
    "        'dask_chunks' : {'x':5000, 'y':5000},\n",
    "    }\n",
    "    \n",
    "    # define features from query\n",
    "    data = gm_mads_two_seasons_prediction(query)\n",
    "\n",
    "    #generate prediction\n",
    "    with HiddenPrints():\n",
    "        predicted = predict_xr(model,data,clean=True).compute()\n",
    "\n",
    "    #-------Post-processsing ------------------------------       \n",
    "    predict = post_processing(predicted)\n",
    "    \n",
    "    #create mask from polygon\n",
    "    mask = xr_rasterize(gdf.iloc[[index]], data)\n",
    "    predict = predict.where(mask)\n",
    "    \n",
    "    #----export classifications to disk-----------------------\n",
    "    col_id = str(gdf.iloc[[index]][column].values[0])\n",
    "    write_cog(predict,\n",
    "              results+output_suffix+'/Southern_cropmask_'+output_suffix+\"_\"+year+\"_\"+col_id+'.tif',\n",
    "              overwrite=True)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
